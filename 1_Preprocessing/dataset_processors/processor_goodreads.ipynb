{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "if np.random.choice(np.arange(1000)) != 102:\n",
    "    raise ValueError(\"Random seed is not set correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa26bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load and preprocess the Goodreads dataset\n",
    "DATASET = 'goodreads'\n",
    "base_artifacts = Path.cwd().resolve().parents[2] / 'CausalI2I_artifacts'\n",
    "data = pd.read_csv(\n",
    "    base_artifacts / 'Datasets' / 'Raw' / DATASET / f'goodreads_filtered.csv'\n",
    ")\n",
    "data.columns = ['user_id', 'item_id', 'timestamp']\n",
    "data['interaction'] = 1\n",
    "data = data.sort_values(by='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae00371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate user-item interactions, keeping only the first occurrence\n",
    "ser = data[['user_id', 'item_id']].value_counts()\n",
    "doubles_list = list(ser[ser > 1].index)\n",
    "for u_id, i_id in doubles_list:\n",
    "    x = data[(data['user_id'] == u_id) & (data['item_id'] == i_id)].index\n",
    "    data = data.drop(x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6efdb39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 7,801\n",
      "Number of unique items: 6,384\n"
     ]
    }
   ],
   "source": [
    "# 2) Get unique users, items, and counts\n",
    "unique_users = data['user_id'].unique()\n",
    "unique_items = data['item_id'].unique()\n",
    "\n",
    "n_users = len(unique_users)\n",
    "n_items = len(unique_items)\n",
    "\n",
    "print(f\"Number of unique users: {n_users:,}\")\n",
    "print(f\"Number of unique items: {n_items:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e774d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Divide users and items into train and test sets\n",
    "test_users = np.random.choice(unique_users, size=int(0.5 * n_users), replace=False)\n",
    "train_users = np.setdiff1d(unique_users, test_users)\n",
    "test_items = np.random.choice(unique_items, size=int(0.2 * n_items), replace=False)\n",
    "train_items = np.setdiff1d(unique_items, test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410afb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of 1's in imputed sets:\n",
      "    Train:  1.75%\n",
      "    Test:   1.70%\n"
     ]
    }
   ],
   "source": [
    "# 4) Create train and test sets\n",
    "all_pairs = pd.merge(\n",
    "    pd.DataFrame({'user_id': unique_users}), \n",
    "    pd.DataFrame({'item_id': unique_items}),\n",
    "    how='cross'\n",
    ")\n",
    "data_imp = pd.merge(\n",
    "    left=all_pairs, \n",
    "    right=data, \n",
    "    on=['user_id', 'item_id'], \n",
    "    how='left'\n",
    ")\n",
    "data_imp = data_imp[['user_id', 'item_id', 'interaction', 'timestamp']]\n",
    "data_imp['interaction'] = data_imp['interaction'].fillna(0).astype(int)\n",
    "\n",
    "train = data_imp[data_imp['user_id'].isin(train_users) | data_imp['item_id'].isin(train_items)].copy()\n",
    "test =  data_imp[data_imp['user_id'].isin(test_users) & data_imp['item_id'].isin(test_items)].copy()\n",
    "print(\"Proportion of 1's in imputed sets:\")\n",
    "print(f\"    Train:  {train['interaction'].mean():.2%}\")\n",
    "print(f\"    Test:   {test['interaction'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b8029fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Reindex users and items to start from 0\n",
    "old2new_users = {old: new for new, old in enumerate(unique_users)}\n",
    "old2new_items = {old: new for new, old in enumerate(unique_items)}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(old2new_users)\n",
    "train['item_id'] = train['item_id'].map(old2new_items)\n",
    "test['user_id'] = test['user_id'].map(old2new_users)\n",
    "test['item_id'] = test['item_id'].map(old2new_items)\n",
    "\n",
    "item_dict = {v: k for k, v in old2new_items.items()}\n",
    "\n",
    "# Data for SASRec\n",
    "data['user_id'] = data['user_id'].map(old2new_users)\n",
    "data['item_id'] = data['item_id'].map(old2new_items)\n",
    "data = data.sort_values(by=['user_id', 'timestamp', 'item_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f973ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving files to disk...\n",
      "Files saved to disk.\n"
     ]
    }
   ],
   "source": [
    "# 6) Save to disk\n",
    "print(\"Saving files to disk...\")\n",
    "train.to_csv(base_artifacts / 'Datasets' / 'Processed' / DATASET / 'train.csv', index=False)\n",
    "test.to_csv(base_artifacts / 'Datasets' / 'Processed' / DATASET / 'test.csv', index=False)\n",
    "data.to_csv(base_artifacts / 'Datasets' / 'Processed' / DATASET / 'data_sasrec.csv', index=False)\n",
    "\n",
    "with open(base_artifacts / 'Datasets' / 'Processed' / DATASET / 'item_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(item_dict, f)\n",
    "\n",
    "print('Files saved to disk.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45dfea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) # Choose 10K pairs\n",
    "full_data = pd.concat([train, test], axis=0)\n",
    "big_pivot = full_data.pivot(index='user_id', columns='item_id', values='interaction')\n",
    "test_items = test['item_id'].unique()\n",
    "\n",
    "X = big_pivot[test_items].values\n",
    "mean = X.mean(axis=0)\n",
    "std  = np.maximum(X.std(axis=0, ddof=1), 1e-8)\n",
    "M = (X - mean) / std\n",
    "corr_mat = (M.T @ M) / (M.shape[0] - 1)\n",
    "np.fill_diagonal(corr_mat, 0)\n",
    "\n",
    "best_flat_pair_idx = corr_mat.flatten().argsort()[-10000:]\n",
    "best_pairs_idx = [(i % len(test_items), i // len(test_items)) for i in best_flat_pair_idx]\n",
    "best_pairs_titles = [(item_dict[test_items[i]], item_dict[test_items[j]]) for i, j in best_pairs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606457a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_artifacts / 'Chosen_Pairs' / f'{DATASET}_chosen_pairs.pkl', \"wb\") as f:\n",
    "    pickle.dump(best_pairs_titles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(base_artifacts / 'Chosen_Pairs' / f'{DATASET}_chosen_pairs.pkl', \"rb\") as f:\n",
    "#     up = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe9e3dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.cwd().resolve().parents[0] / '2_Propensities'))\n",
    "import MF_class as MF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c01844",
   "metadata": {},
   "source": [
    "# 1 Choosing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f89981e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: ml-1m\n"
     ]
    }
   ],
   "source": [
    "datasets = ['ml-1m', 'steam', 'goodreads']\n",
    "DATASET = datasets[0]\n",
    "\n",
    "print(f\"Using dataset: {DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43408a1a",
   "metadata": {},
   "source": [
    "# 2 Loading Dataset and Propensities Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61333dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_artifacts = Path.cwd().resolve().parents[1] / 'CausalI2I_artifacts'\n",
    "path = base_artifacts / 'Datasets' / 'Processed' / DATASET\n",
    "train = pd.read_csv(path / 'train.csv')\n",
    "test = pd.read_csv(path / 'test.csv')\n",
    "with open(path / 'item_dict.pkl', 'rb') as f:\n",
    "    item_dict = pickle.load(f)\n",
    "\n",
    "n_users = len(train['user_id'].unique())\n",
    "n_items = len(item_dict)\n",
    "\n",
    "full_data = pd.concat([train, test], ignore_index=True)\n",
    "title2id = {v: k for k, v in item_dict.items()}\n",
    "\n",
    "# Load chosen pairs and item dictionary\n",
    "with open(base_artifacts / 'Chosen_Pairs' / f'{DATASET}_chosen_pairs.pkl', 'rb') as f:\n",
    "    chosen_pairs = pickle.load(f)\n",
    "\n",
    "chosen_pairs_ids = [\n",
    "    (title2id[title_A], title2id[title_B])\n",
    "    for title_A, title_B in chosen_pairs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0993e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model summary:\n",
      "Model:                      MatrixFactorizationTorch\n",
      "Number of users:            6040\n",
      "Number of items:            3706\n",
      "Number of factors:          20\n",
      "Learning rate:              0.005\n",
      "Weight decay:               1e-07\n",
      "Positive weight:            1\n",
      "Batch size:                 32768\n",
      "Number of epochs:           20\n",
      "Device:                     cuda:0\n",
      "Use AMP:                    True\n",
      "Timestamp:                  2025-12-19 16:50:42\n"
     ]
    }
   ],
   "source": [
    "model_path = base_artifacts / 'Propensity_Models'\n",
    "all_model_names = [p.name for p in model_path.iterdir() if p.is_file()]\n",
    "model_name = [name for name in all_model_names if DATASET in name][0]\n",
    "n_factors = int(model_name.split('_')[0][2:])\n",
    "\n",
    "model = MF.MatrixFactorizationTorch(n_users=n_users, n_items=n_items, n_factors=n_factors)\n",
    "model.load(path=model_path / model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116a615",
   "metadata": {},
   "source": [
    "# 3 Processing ChatGPT Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73ae08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = base_artifacts / 'API_Results' / DATASET\n",
    "csvs_in_folder = [p.name for p in folder.iterdir() if p.is_file() and p.suffix == '.csv']\n",
    "oracle_file_name = csvs_in_folder[0]\n",
    "oracle = pd.read_csv(folder / oracle_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ff332bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling 2 pairs with unknown titles.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>causal_effect</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "causal_effect       \n",
       "0               9862\n",
       "1                 84\n",
       "2                 34\n",
       "3                 20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_string = lambda s: s[1:-1] if s[0] == \"'\" and s[-1] == \"'\" else s\n",
    "oracle['title_A'] = oracle['title_A'].apply(clean_string)\n",
    "oracle['title_B'] = oracle['title_B'].apply(clean_string)\n",
    "\n",
    "known_titles = title2id.keys()\n",
    "oracle['titles_known'] = oracle['title_A'].apply(lambda x: x in known_titles) & oracle['title_B'].apply(lambda x: x in known_titles)\n",
    "print(f\"Filling {(oracle['titles_known'] == False).sum()} pairs with unknown titles.\")\n",
    "\n",
    "filling_idx = oracle[oracle['titles_known'] == False].index\n",
    "for idx in filling_idx:\n",
    "    oracle.loc[idx, 'title_A'] = chosen_pairs[idx][0]\n",
    "    oracle.loc[idx, 'title_B'] = chosen_pairs[idx][1]\n",
    "\n",
    "pd.DataFrame(oracle['causal_effect'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9f20f",
   "metadata": {},
   "source": [
    "# 4 Defining Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56157fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_real = full_data.pivot(index='user_id', columns='item_id', values='interaction').fillna(0)\n",
    "itemid_to_colidx_pivot_real = {item_id: col_idx for col_idx, item_id in enumerate(pivot_real.columns)}\n",
    "pivot_real_np = pivot_real.values\n",
    "\n",
    "Q_normalized = (model.Q / torch.norm(model.Q, dim=1, keepdim=True)).cpu().detach().numpy()\n",
    "\n",
    "def cosimilarity(idx1, idx2):\n",
    "    \"\"\"Calculate cosine similarity between two items.\"\"\"\n",
    "    return np.dot(Q_normalized[idx1], Q_normalized[idx2])\n",
    "\n",
    "def correlation(idx1, idx2):\n",
    "    \"\"\"Calculate correlation between two items.\"\"\"\n",
    "    colidx1 = itemid_to_colidx_pivot_real[idx1]\n",
    "    colidx2 = itemid_to_colidx_pivot_real[idx2]\n",
    "    T = pivot_real_np[:, colidx1]\n",
    "    Y = pivot_real_np[:, colidx2]\n",
    "    if T.std() == 0 or Y.std() == 0:\n",
    "        return 0\n",
    "    return np.corrcoef(T, Y)[0, 1]\n",
    "\n",
    "def diff_of_conditionals(idx1, idx2):\n",
    "    \"\"\"Calculate difference of conditionals P(Y|T) - P(Y|~T) between two items.\"\"\"\n",
    "    colidx1 = itemid_to_colidx_pivot_real[idx1]\n",
    "    colidx2 = itemid_to_colidx_pivot_real[idx2]\n",
    "    T = pivot_real_np[:, colidx1]\n",
    "    Y = pivot_real_np[:, colidx2]\n",
    "    p_T = np.clip(np.mean(T), 1e-6, 1-1e-6)\n",
    "    p_Y = np.mean(Y)\n",
    "    p_TY = np.mean(T * Y)\n",
    "    return p_TY / p_T - (p_Y - p_TY) / (1 - p_T)\n",
    "\n",
    "def jacard_index(idx1, idx2):\n",
    "    \"\"\"Calculate Jaccard index between two items.\"\"\"\n",
    "    colidx1 = itemid_to_colidx_pivot_real[idx1]\n",
    "    colidx2 = itemid_to_colidx_pivot_real[idx2]\n",
    "    T = pivot_real_np[:, colidx1]\n",
    "    Y = pivot_real_np[:, colidx2]\n",
    "    intersection = np.sum((T > 0) & (Y > 0))\n",
    "    union = np.sum((T > 0) | (Y > 0))\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2cd82",
   "metadata": {},
   "source": [
    "## 4.1 SASRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83dca158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/gouni/CausalI2I_artifacts/SASRec_Models/sasrec_ml-1m.pt.\n",
      "num_items:     3706\n",
      "max_seq_len:   200\n",
      "device:        cuda\n",
      "batch_size:    128\n",
      "lr:            0.001\n",
      "weight_decay:  0.0\n",
      "num_epochs:    10\n",
      "saved_at:      2025-12-22 18:23:04\n",
      "note:          None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gouni/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, str(Path.cwd().resolve().parents[0] / '4_SASRec'))\n",
    "import SASRec_class as sasrec\n",
    "\n",
    "model_path = base_artifacts / 'SASRec_Models'\n",
    "with open(model_path / f'sasrec_{DATASET}_init_dict.pkl', 'rb') as f:\n",
    "    init_dict_loaded = pickle.load(f)\n",
    "sasrec_model = sasrec.SASRecTorch(**init_dict_loaded)\n",
    "sasrec_model.load(model_path / f'sasrec_{DATASET}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "879138b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequence(cause_id):\n",
    "    PAD = sasrec_model.num_items\n",
    "    L = sasrec_model.max_seq_len\n",
    "    seq = torch.full((1, L), PAD, dtype=torch.long, device=sasrec_model.device)\n",
    "    seq[0, -1] = cause_id\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ca0d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc0a58cc9764a68b7ef03a082305746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sasrec_model.eval()\n",
    "\n",
    "sasrec_scores = {}\n",
    "sasrec_ranks = {}\n",
    "candidates = torch.arange(0, sasrec_model.num_items, device=sasrec_model.device)\n",
    "for pair in tqdm(chosen_pairs_ids):\n",
    "    cause_id, effect_id = pair\n",
    "    seq = make_sequence(cause_id)\n",
    "    candidates_scores = sasrec_model.predict_scores(seq, candidates).detach().cpu().numpy()[0]\n",
    "    # Record Score\n",
    "    sasrec_scores[pair] = candidates_scores[effect_id]\n",
    "    # Record Rank\n",
    "    rank = np.where(candidates_scores.argsort()[::-1] == effect_id)[0][0]   # 0-based rank\n",
    "    percentile = 1 - rank / sasrec_model.num_items\n",
    "    sasrec_ranks[pair] = percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14761b32",
   "metadata": {},
   "source": [
    "# 5 Defining ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab22b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_prob(\n",
    "        torch.tensor(test['user_id'].values, dtype=torch.long),\n",
    "        torch.tensor(test['item_id'].values, dtype=torch.long)\n",
    "    )\n",
    "test_copy = test.copy()\n",
    "if test_copy['timestamp'].dtype == 'O':\n",
    "    test_copy['timestamp'] = pd.to_datetime(test_copy['timestamp'], errors='coerce').astype(np.int64) // 10**9\n",
    "    test_copy['timestamp'] = test_copy['timestamp'].apply(lambda x: x if x > 0 else np.inf)\n",
    "\n",
    "pivot_test_timestamp = test_copy.pivot(index='user_id', columns='item_id', values='timestamp').fillna(np.inf)\n",
    "pivot_test_timestamp_np = pivot_test_timestamp.values\n",
    "itemid_to_colidx = {id: i for i, id in enumerate(pivot_test_timestamp.columns)}\n",
    "\n",
    "test_copy['probability'] = test_probs.cpu().detach().numpy()\n",
    "pivot_test_pred = test_copy.pivot(index='user_id', columns='item_id', values='probability')\n",
    "pivot_test_pred_np = pivot_test_pred.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eabccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interaction_time_cols  = {\n",
    "    item: pivot_test_timestamp_np[:, colidx]\n",
    "    for item, colidx in itemid_to_colidx.items()\n",
    "}\n",
    "\n",
    "pred_cols = {\n",
    "    item: pivot_test_pred_np[:, colidx]\n",
    "    for item, colidx in itemid_to_colidx.items()\n",
    "}\n",
    "\n",
    "all_interaction_cols = {\n",
    "    item: pivot_real_np[:, colidx]\n",
    "    for item, colidx in itemid_to_colidx_pivot_real.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8d1d40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ATE(cause_times, effect_times, pi, clip=0, drop_inverted=True, stabilized=True):\n",
    "    \"\"\"\n",
    "    If stabilized=True  -> Hájek ratio IPW: E[Y(1)] ≈ mean(N1)/mean(D1), E[Y(0)] ≈ mean(N0)/mean(D0)\n",
    "    If stabilized=False -> Horvitz–Thompson IPW:   E[Y(1)] ≈ mean(N1),    E[Y(0)] ≈ mean(N0)\n",
    "    Returns (ATE, STD) unless return_STD=False.\n",
    "    \"\"\"\n",
    "\n",
    "    if drop_inverted:\n",
    "        users_to_keep = np.where((cause_times <= effect_times) | (cause_times == np.inf))[0]\n",
    "\n",
    "        cause_time_filtered = cause_times[users_to_keep]\n",
    "        effect_time_filtered = effect_times[users_to_keep]\n",
    "\n",
    "        T = cause_time_filtered < np.inf\n",
    "        Y = effect_time_filtered < np.inf\n",
    "        pi = pi[users_to_keep]\n",
    "\n",
    "    else:\n",
    "        T  = cause_times < np.inf\n",
    "        Y  = effect_times < np.inf\n",
    "    \n",
    "    n   = len(T)\n",
    "\n",
    "    pi = np.clip(pi, clip, 1 - clip)\n",
    "\n",
    "    # IPW pieces\n",
    "    D_1 = T / pi                # B\n",
    "    D_0 = (1 - T) / (1 - pi)    # D\n",
    "    N_1 = Y * D_1               # A\n",
    "    N_0 = Y * D_0               # C\n",
    "    mN_1, mN_0 = N_1.mean(), N_0.mean()\n",
    "\n",
    "    # Point estimate\n",
    "    if stabilized:\n",
    "        # Hájek (ratio) form\n",
    "        mD_1, mD_0 = D_1.mean(), D_0.mean()\n",
    "        EY_1 = mN_1 / mD_1 if mD_1 != 0 else 0.0\n",
    "        EY_0 = mN_0 / mD_0 if mD_0 != 0 else 0.0\n",
    "    else:\n",
    "        # Horvitz–Thompson (mean) form\n",
    "        EY_1 = mN_1\n",
    "        EY_0 = mN_0\n",
    "\n",
    "    ATE = EY_1 - EY_0\n",
    "\n",
    "    # ---- Variance via explicit covariance matrix ----\n",
    "    if stabilized:\n",
    "        Z = np.column_stack([N_1, D_1, N_0, D_0])\n",
    "        S = np.cov(Z, rowvar=False, ddof=1)     # 4x4 sample covariance\n",
    "        g = np.array([\n",
    "            1.0 / mD_1 if mD_1 != 0 else 0.0,\n",
    "            -mN_1 / (mD_1 ** 2) if mD_1 != 0 else 0.0,\n",
    "            -1.0 / mD_0 if mD_0 != 0 else 0.0,\n",
    "            mN_0 / (mD_0 ** 2) if mD_0 != 0 else 0.0\n",
    "        ])\n",
    "    else:\n",
    "        Z = np.column_stack([N_1, N_0])\n",
    "        S = np.cov(Z, rowvar=False, ddof=1)     # 2x2 covariance\n",
    "        g = np.array([1.0, -1.0])\n",
    "    \n",
    "    var_hat = (g @ S @ g) / n\n",
    "    STD = float(np.sqrt(max(var_hat, 0.0)))     # numerical safety\n",
    "\n",
    "    return {\n",
    "        \"ATE\": ATE,\n",
    "        \"STD\": STD,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4f110",
   "metadata": {},
   "source": [
    "# 6 Generate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b38c3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dict = {\n",
    "    'ml-1m': 0.1,\n",
    "    'steam': 0.01,\n",
    "    'goodreads': 0.01,\n",
    "}\n",
    "clip = clip_dict[DATASET]\n",
    "\n",
    "def process_pair(pair):\n",
    "\n",
    "    c = pair[0]\n",
    "    e = pair[1]\n",
    "\n",
    "    ate_dict = get_ATE(\n",
    "        cause_times=test_interaction_time_cols[c], \n",
    "        effect_times=test_interaction_time_cols[e], \n",
    "        pi=pred_cols[c], \n",
    "        clip=clip,\n",
    "        drop_inverted=True, \n",
    "        stabilized=True)\n",
    "    \n",
    "    abl_dict = get_ATE(\n",
    "        cause_times=test_interaction_time_cols[c], \n",
    "        effect_times=test_interaction_time_cols[e], \n",
    "        pi=pred_cols[c], \n",
    "        clip=0.5, \n",
    "        drop_inverted=True, \n",
    "        stabilized=True)\n",
    "\n",
    "    return {\n",
    "        \"cause_id\": pair[0],\n",
    "        \"effect_id\": pair[1],\n",
    "        \"ATE\": ate_dict[\"ATE\"],\n",
    "        \"STD\": ate_dict[\"STD\"],\n",
    "        \"ABLT\": abl_dict[\"ATE\"],\n",
    "        \"STD_ABLT\": abl_dict[\"STD\"],\n",
    "        \"cosine_similarity\": cosimilarity(*pair),\n",
    "        \"correlation\": correlation(*pair),\n",
    "        \"diff_of_conditionals\": diff_of_conditionals(*pair),\n",
    "        \"jacard_index\": jacard_index(*pair),\n",
    "        \"sasrec_score\": sasrec_scores[pair],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef32a0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469bfe1b467e48a08eb042a96f67494e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = []\n",
    "for pair in tqdm(chosen_pairs_ids):\n",
    "    results = process_pair(pair)\n",
    "    all_results.append(results)\n",
    "\n",
    "raw_results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06abe2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(\n",
    "    left=oracle,\n",
    "    right=raw_results,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d47da1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(base_artifacts / 'Datasets' / 'Evaluated' / f'{DATASET}_evaluated.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
